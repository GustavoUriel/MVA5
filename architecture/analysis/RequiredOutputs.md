# 00_RequiredOutputs.md: Complete Pipeline Output Specifications

This document specifies all outputs that should be generated by the microbiome analysis pipeline, including visualizations, tables, and reports for both single pipeline runs and multi-run comparisons.

---

## üéØ **Document Purpose**

This specification defines:
- **List 1:** Complete outputs for a single pipeline run
- **List 2:** Comparison outputs for multiple pipeline runs
- How to read and interpret each output
- Why each output is necessary
- Output format specifications

**Use this document when:** Planning pipeline implementation, validating outputs, or documenting results for publication.

---

# üìä **LIST 1: Single Pipeline Run Outputs**

These outputs are generated from one complete execution of the pipeline (all 7 stages).

---

## üî¨ **Category 1: Data Quality & Curation Outputs**

### **1.1 Data Quality Report**
**File:** `01_data_quality_report.pdf`  
**Type:** PDF Report

**What It Shows:**
- Pre- and post-filtering sample/taxa counts
- Library size distributions (histogram)
- Rarefaction curves per sample
- Contaminants identified and removed
- Quality control metrics

**How to Read:**
- Check if library sizes are adequate (typically >10,000 reads)
- Verify rarefaction curves plateau (adequate sequencing depth)
- Review removed contaminants list for expected contaminants (e.g., skin bacteria)
- Ensure sufficient samples/taxa remain after filtering

**Why It's Needed:**
Essential for validating data quality and justifying QC decisions in methods section. Identifies potential batch effects or technical issues.

---

### **1.2 Normalization Summary Table**
**File:** `01_normalization_summary.csv`

**Columns:**
```
Normalization_Method, Parameters, Pre_Normalization_Stats, Post_Normalization_Stats, Transformation_Type
```

**What It Shows:**
Detailed information about normalization applied (CLR, relative abundance, etc.), including parameters and statistical effects.

**How to Read:**
- Check normalization method matches study design
- Verify transformation stabilized variance
- Compare before/after distributions

**Why It's Needed:**
Documents preprocessing decisions for reproducibility. Critical for methods section and data sharing.

---

### **1.3 Sample Metadata Summary**
**File:** `01_sample_metadata_cleaned.csv`

**Columns:**
```
Sample_ID, Patient_ID, Timepoint, Clinical_Variables, PFS_Time, PFS_Event, QC_Pass, Exclusion_Reason
```

**What It Shows:**
Complete cleaned clinical metadata with QC flags and exclusion reasons.

**How to Read:**
- Review exclusion reasons for samples removed
- Check clinical variable completeness
- Verify PFS data availability

**Why It's Needed:**
Documents which samples were included/excluded. Essential for clinical interpretation and reproducibility.

---

## üß¨ **Category 2: Microbial Grouping Outputs**

### **2.1 Group Definitions Table**
**File:** `02_microbial_group_definitions.csv`

**Columns:**
```
Group_Name, Description, Selection_Criteria, N_Taxa, Taxa_List, Literature_References, Expected_PFS_Effect
```

**Example Rows:**
```
SGroup1, SCFA producers, literature_based, 45, [Faecalibacterium; Roseburia; ...], Smith2020; Jones2021, protective
SGroup2, Pathogenic bacteria, literature_based, 23, [Escherichia; Klebsiella; ...], Brown2019, risk
SGroup3, Bile acid metabolizers, literature_based, 31, [Clostridium; ...], Davis2018, protective
```

**How to Read:**
- Review biological rationale for each group
- Check taxa count (groups with very few taxa may be unreliable)
- Verify expected PFS effect based on literature
- Cross-reference with citation

**Why It's Needed:**
Documents the biological basis for grouping. Essential for interpreting results in functional context and justifying grouping strategy.

---

### **2.2 Taxa Group Assignment Table**
**File:** `02_taxa_group_assignments.csv`

**Columns:**
```
Taxon_Name, Assigned_Groups, Confidence_Scores, Assignment_Method, Prevalence, Mean_Abundance
```

**Example Rows:**
```
Faecalibacterium_prausnitzii, SGroup1|SGroup3, 0.95|0.70, literature_based, 0.85, 0.025
Bifidobacterium_longum, SGroup1, 0.90, literature_based, 0.78, 0.018
```

**How to Read:**
- Multiple groups per taxon indicated by pipe separator (|)
- Confidence scores indicate assignment certainty (>0.80 = high confidence)
- Prevalence shows % of samples with taxon present
- Mean abundance in relative units

**Why It's Needed:**
Provides traceability for which taxa belong to which groups. Enables validation of assignments and identification of taxa with multiple functions.

---

### **2.3 Group Coverage Venn Diagram**
**File:** `02_group_overlap_venn.png`  
**Type:** PNG Image

**What It Shows:**
Venn diagram showing overlap between microbial groups (SGroup1, SGroup2, SGroup3).

**How to Read:**
- Overlapping regions show taxa assigned to multiple groups
- Size of regions proportional to number of taxa
- Numbers indicate taxa count in each region

**Why It's Needed:**
Visualizes functional redundancy and multi-functional taxa. Helps identify if groups are independent or overlapping.

---

### **2.4 Group Abundance Summary**
**File:** `02_group_abundance_summary.csv`

**Columns:**
```
Group_Name, Mean_Abundance, Std_Abundance, Prevalence, Min, Max, Median, Timepoint
```

**How to Read:**
- Mean abundance shows average relative abundance of group
- Prevalence indicates % of samples with ‚â•1 taxon from group
- Compare across timepoints if longitudinal data

**Why It's Needed:**
Provides baseline statistics for each microbial group. Essential for understanding group representation in dataset.

---

## üéØ **Category 3: Variable Selection Outputs**

### **3.1 Variable Selection Results Table**
**File:** `03_variable_selection_results.csv`

**Columns:**
```
Taxon_Name, Method1_Decision, Method1_Score, Method1_Criteria, Method2_Decision, Method2_Score, Method2_Criteria, Final_Decision, Decision_Reason
```

**Example Rows:**
```
Faecalibacterium_prausnitzii, included, 0.85, prevalence>0.10, included, 0.02, p<0.10, included, passed_all_filters
Rare_taxon_X, excluded, 0.03, prevalence>0.10, not_evaluated, NA, NA, excluded, failed_prevalence_filter
```

**How to Read:**
- Check which methods were applied (Method1, Method2, etc.)
- "included" = taxon passed filter; "excluded" = failed
- Score interpretation depends on method (prevalence = %, p-value, etc.)
- Final_Decision shows consensus across methods

**Why It's Needed:**
Documents which taxa passed selection and why. Critical for transparency and methods justification. Helps identify borderline taxa.

---

### **3.2 Method Comparison Matrix**
**File:** `03_selection_method_comparison.csv`

**Columns:**
```
Method1, Method2, N_Method1_Selected, N_Method2_Selected, N_Overlap, Overlap_Percentage, Unique_to_Method1, Unique_to_Method2
```

**How to Read:**
- Overlap_Percentage shows agreement between methods (70%+ = good agreement)
- Unique lists show method-specific selections
- Low overlap (<50%) suggests methods targeting different criteria

**Why It's Needed:**
Assesses consistency across selection approaches. High agreement increases confidence in selected features.

---

### **3.3 Selection Method Agreement Heatmap**
**File:** `03_method_agreement_heatmap.png`  
**Type:** PNG Image (Heatmap)

**What It Shows:**
Color-coded matrix showing pairwise agreement between selection methods. Rows/columns = methods, cells = overlap percentage.

**How to Read:**
- Diagonal = 100% (method agrees with itself)
- Darker colors = higher agreement
- Look for clusters of high agreement (suggests consistent selection)
- Off-diagonal light colors = method disagreement

**Why It's Needed:**
Visual overview of method consistency. Identifies which methods produce similar vs. different results.

---

### **3.4 Taxa Exclusion Reasons Barplot**
**File:** `03_exclusion_reasons_barplot.png`  
**Type:** PNG Image (Barplot)

**What It Shows:**
Bar chart showing counts of taxa excluded for each reason (low_prevalence, low_abundance, no_PFS_association, etc.).

**How to Read:**
- X-axis: Exclusion reasons
- Y-axis: Number of taxa
- Tallest bars indicate most common exclusion criteria

**Why It's Needed:**
Summarizes why taxa were removed. Helps validate that exclusion criteria were appropriate and not too stringent.

---

### **3.5 Selected Features List**
**File:** `03_selected_features_final.txt`

**Format:**
```
# Final Selected Taxa for Downstream Analysis
# Total: 85 taxa
# Selection Strategy: intersection of prevalence_filter + pfs_univariate

Faecalibacterium_prausnitzii
Bifidobacterium_longum
Lactobacillus_rhamnosus
...
```

**How to Read:**
Simple list of taxa names that passed all filters.

**Why It's Needed:**
Provides quick reference for which taxa proceed to grouping/MVA. Useful for reproducibility and communication.

---

## üîó **Category 4: Variable Grouping Outputs**

### **4.1 Composite Features Table**
**File:** `04_composite_features_details.csv`

**Columns:**
```
Composite_Name, N_Taxa, Constituent_Taxa, Aggregation_Method, Variance_Explained, Individual_Contributions, Biological_Interpretation
```

**Example Rows:**
```
Cluster_Beneficial_Bacteria, 4, Faecalibacterium|Bifidobacterium|Lactobacillus|Akkermansia, mean_abundance, 0.72, 0.40|0.30|0.18|0.12, SCFA-producing beneficial bacteria
Cluster_Pathogens, 3, Escherichia|Klebsiella|Enterococcus, pca_first_component, 0.65, 0.55|0.45|0.35, Opportunistic pathogens
```

**How to Read:**
- Constituent_Taxa shows which taxa merged (pipe-separated)
- Variance_Explained = how well composite represents constituents (>0.60 = good)
- Individual_Contributions = relative weight of each taxon (pipe-separated, order matches Constituent_Taxa)
- Aggregation_Method = how taxa combined (mean, PCA, sum)

**Why It's Needed:**
Documents which taxa were grouped together and how. Essential for understanding composite features and interpreting MVA results.

---

### **4.2 Composite Constituent Radar Plots**
**File:** `04_composite_radar_plots.png`  
**Type:** PNG Image (Multi-panel radar plots)

**What It Shows:**
Radar plot for each composite showing individual taxa contributions. Each axis = one constituent taxon, distance from center = contribution strength.

**How to Read:**
- Larger polygons = more balanced contributions
- Pointed polygons = dominated by 1-2 taxa
- Compare polygon shape to variance_explained (balanced shapes usually have higher variance explained)

**Why It's Needed:**
Visualizes how taxa contribute to composites. Helps identify if composites are dominated by single taxa or balanced.

---

### **4.3 Correlation Matrix Within Composites**
**File:** `04_correlation_matrices.png`  
**Type:** PNG Image (Multi-panel heatmaps)

**What It Shows:**
Separate correlation heatmap for taxa within each composite. Shows pairwise correlations between constituent taxa.

**How to Read:**
- Color intensity = correlation strength
- Positive correlations (red/orange) = taxa co-occur
- Negative correlations (blue) = taxa mutually exclusive
- High correlations justify grouping; low correlations suggest poor grouping

**Why It's Needed:**
Validates that grouped taxa are actually correlated. High within-group correlations justify the aggregation strategy.

---

### **4.4 Variance Explained Table**
**File:** `04_variance_explained_summary.csv`

**Columns:**
```
Composite_Name, PC1_Variance, PC2_Variance, Total_Variance_Explained, N_Components_Needed_90pct
```

**How to Read:**
- PC1_Variance = % variance explained by first principal component
- Total_Variance_Explained = cumulative variance captured
- N_Components_Needed_90pct = dimensionality reduction effectiveness

**Why It's Needed:**
Quantifies information loss from grouping. High variance explained (>70%) means little information lost by aggregation.

---

## üéñÔ∏è **Category 5: Group Selection Outputs**

### **5.1 Group Selection Results Table**
**File:** `05_group_selection_results.csv`

**Columns:**
```
Composite_Name, Method1_Decision, Method1_Score, Method1_Pvalue, Method2_Decision, Method2_Score, Final_Decision, Importance_Rank
```

**How to Read:**
- Similar to variable selection table but for composites
- Importance_Rank = overall ranking (1 = most important)
- Check consistency across methods

**Why It's Needed:**
Documents which composites passed selection for MVA. Justifies final feature set.

---

### **5.2 Composite Importance Barplot**
**File:** `05_composite_importance_barplot.png`  
**Type:** PNG Image (Barplot)

**What It Shows:**
Bar chart showing importance scores for all composites, colored by selection decision (selected = green, excluded = red).

**How to Read:**
- X-axis: Composite names
- Y-axis: Importance score
- Green bars = selected for MVA
- Red bars = excluded
- Dashed line = selection threshold

**Why It's Needed:**
Visual summary of group selection decisions. Shows relative importance of composites and selection cutoff.

---

### **5.3 Final Feature Set Summary**
**File:** `05_final_features_summary.txt`

**Format:**
```
# Final Features for MVA
# Total: 8 features (3 composites + 5 clinical variables)

COMPOSITE FEATURES:
- Cluster_Beneficial_Bacteria (4 taxa)
- Cluster_Pathogens (3 taxa)
- Cluster_SCFA_Producers (6 taxa)

CLINICAL FEATURES:
- Age
- Gender
- ISS_Stage
- Beta2_Microglobulin
- Treatment_Type
```

**How to Read:**
Lists all features (composite + clinical) that will be used in MVA models.

**Why It's Needed:**
Clear documentation of final feature set. Essential for methods section and reproducibility.

---

## üìà **Category 6: MVA Methods Outputs**

### **6.1 Cox Regression Results Table**
**File:** `06_cox_regression_results.csv`

**Columns:**
```
Feature, Coefficient, Exp(Coef), Std_Error, P_Value, HR_Lower_95CI, HR_Upper_95CI, Significance
```

**How to Read:**
- Exp(Coef) = Hazard Ratio (HR)
  - HR > 1: increased risk (higher values = worse PFS)
  - HR < 1: decreased risk (protective)
  - HR = 1: no effect
- P_Value < 0.05 = statistically significant
- 95% CI = uncertainty range (wider = less precise)

**Why It's Needed:**
Primary survival analysis results. Shows which features predict PFS and direction/magnitude of effect. Gold standard for survival analysis.

---

### **6.2 Cox Regression Forest Plot**
**File:** `06_cox_forest_plot.png`  
**Type:** PNG Image (Forest plot)

**What It Shows:**
Visual representation of hazard ratios with confidence intervals. Each row = one feature, point = HR estimate, horizontal line = 95% CI.

**How to Read:**
- Vertical line at HR = 1 is null effect
- Points left of line = protective (HR < 1)
- Points right of line = risk (HR > 1)
- Lines crossing vertical line = not significant
- Wider lines = more uncertainty

**Why It's Needed:**
Most common way to present Cox regression results. Essential for publication. Provides intuitive visual of effect sizes and significance.

---

### **6.3 Cox Regression P-Values Plot**
**File:** `06_cox_pvalues_plot.png`  
**Type:** PNG Image (Barplot/Lollipop)

**What It Shows:**
Bar/lollipop chart showing -log10(p-values) for each feature. Higher bars = more significant.

**How to Read:**
- Y-axis: -log10(p-value)
- Horizontal dashed line at 1.3 = p=0.05 threshold
- Bars above line = significant
- Taller bars = stronger significance

**Why It's Needed:**
Quick visual identification of significant features. Complements forest plot with significance focus.

---

### **6.4 Random Forest Results Table**
**File:** `06_random_forest_results.csv`

**Columns:**
```
Feature, Importance_Score, Permutation_Importance, Rank, Std_Error
```

**How to Read:**
- Importance_Score = feature contribution to model (0-1 scale)
- Higher scores = more important features
- Permutation_Importance = validated importance after shuffling
- Rank = overall importance ranking (1 = most important)

**Why It's Needed:**
Non-parametric alternative to Cox. Captures non-linear relationships. Importance scores show feature relevance without assuming proportional hazards.

---

### **6.5 Random Forest Feature Importance Plot**
**File:** `06_rf_importance_plot.png`  
**Type:** PNG Image (Barplot)

**What It Shows:**
Horizontal bar chart showing feature importance scores from random forest.

**How to Read:**
- X-axis: Importance score (0-1)
- Y-axis: Features (sorted by importance)
- Longer bars = more important features
- Error bars show uncertainty from cross-validation

**Why It's Needed:**
Visual ranking of feature importance. Helps identify top predictive features and compare to Cox results.

---

### **6.6 Model Performance Comparison Table**
**File:** `06_model_performance_comparison.csv`

**Columns:**
```
Model, AUROC, C_Index, Brier_Score, AIC, BIC, CV_Mean_AUROC, CV_Std_AUROC, N_Features
```

**Example:**
```
Cox_Regression, 0.82, 0.79, 0.16, 498.6, 512.4, 0.80, 0.04, 8
Random_Forest, 0.85, 0.82, 0.14, NA, NA, 0.83, 0.05, 8
Neural_Network, 0.87, 0.84, 0.12, NA, NA, 0.85, 0.06, 8
```

**How to Read:**
- AUROC: Area Under ROC Curve (0.5-1.0, higher = better discrimination)
  - 0.5 = random chance
  - 0.7-0.8 = acceptable
  - 0.8-0.9 = excellent
  - >0.9 = outstanding (or overfitting)
- C_Index: Concordance index (similar to AUROC)
- Brier_Score: Calibration metric (0-1, lower = better calibration)
- AIC/BIC: Model selection criteria (lower = better fit, only for Cox)
- CV metrics: Cross-validation results (mean ¬± std)

**Why It's Needed:**
Compares predictive performance across methods. Essential for choosing best model and reporting model quality.

---

### **6.7 ROC Curves Plot**
**File:** `06_roc_curves_comparison.png`  
**Type:** PNG Image (ROC curves)

**What It Shows:**
ROC curves for all MVA methods on same plot. X-axis = False Positive Rate, Y-axis = True Positive Rate.

**How to Read:**
- Diagonal dashed line = random chance (AUROC = 0.5)
- Curves further from diagonal = better performance
- Legend shows AUROC for each method
- Compare curve positions to identify best model

**Why It's Needed:**
Visual comparison of discriminative ability across methods. Standard way to present classification performance.

---

### **6.8 Calibration Plot**
**File:** `06_survival_calibration_plot.png`  
**Type:** PNG Image (Calibration curve)

**What It Shows:**
Predicted vs. observed survival probabilities. Perfect calibration = diagonal line.

**How to Read:**
- X-axis: Predicted probability of event
- Y-axis: Observed frequency of event
- Points close to diagonal = well calibrated
- Points above diagonal = overestimation
- Points below diagonal = underestimation

**Why It's Needed:**
Validates that predicted probabilities match reality. Essential for clinical application where accurate risk estimates needed.

---

### **6.9 Feature Importance Comparison Heatmap**
**File:** `06_feature_importance_heatmap.png`  
**Type:** PNG Image (Heatmap)

**What It Shows:**
Heatmap with features as rows, methods as columns, cells colored by importance ranking or score.

**How to Read:**
- Darker colors = higher importance
- Look for features with dark colors across all methods (consistent importance)
- Light colors in one method but dark in others = method-specific importance

**Why It's Needed:**
Identifies features that are important across methods (more robust) vs. method-specific (less reliable).

---

## üèÜ **Category 7: Consensus Analysis Outputs**

### **7.1 Consensus Ranking Table**
**File:** `07_consensus_ranking_table.csv`

**Columns:**
```
Rank, Taxon_Name, Consensus_Score, Cox_Score, RF_Score, NN_Score, Method_Count, Confidence, Evidence_Strength, Clinical_Interpretation
```

**Example:**
```
1, Faecalibacterium_prausnitzii, 0.78, 0.85, 0.25, 0.32, 4, 1.0, strong, Strong protective factor
2, Bifidobacterium_longum, 0.71, 0.80, 0.18, 0.28, 3, 0.75, moderate, Moderate protective factor
```

**How to Read:**
- Consensus_Score = weighted average across all methods (0-1 scale)
- Individual method scores show contribution from each method
- Method_Count = how many methods evaluated this taxon (4 = all methods)
- Confidence = Method_Count / 4 (fraction of methods)
- Evidence_Strength = strong/moderate/weak/insufficient
  - Strong: consensus ‚â•0.75, confidence ‚â•0.75
  - Moderate: consensus ‚â•0.60, confidence ‚â•0.50
  - Weak: consensus ‚â•0.40
  - Insufficient: consensus <0.40

**Why It's Needed:**
**Most important table for identifying top biomarkers.** Integrates all methods into single robust ranking. Essential for prioritizing taxa for validation studies.

---

### **7.2 Consensus Ranking Barplot**
**File:** `07_consensus_ranking_barplot.png`  
**Type:** PNG Image (Barplot with error bars)

**What It Shows:**
Top 10-20 taxa ranked by consensus score with confidence intervals. Bars colored by evidence strength (green = strong, yellow = moderate, orange = weak).

**How to Read:**
- X-axis: Consensus score
- Y-axis: Taxa names (sorted by rank)
- Error bars = confidence from bootstrap
- Bar color = evidence strength
- Focus on green bars (strong evidence)

**Why It's Needed:**
Visual representation of consensus rankings. Perfect for presentations and publications. Shows both ranking and uncertainty.

---

### **7.3 Method Agreement Analysis Table**
**File:** `07_method_agreement_table.csv`

**Columns:**
```
Agreement_Level, Taxa_Count, Taxa_List
```

**Example:**
```
Perfect_Agreement, 12, Faecalibacterium|Bifidobacterium|...
High_Agreement, 18, Lactobacillus|Roseburia|...
Moderate_Agreement, 25, ...
Low_Agreement, 15, ...
```

**How to Read:**
- Perfect_Agreement = taxa in top-N of ALL methods (most robust)
- High_Agreement = taxa in top-N of ‚â•75% methods
- Moderate_Agreement = taxa in top-N of ‚â•50% methods
- Low_Agreement = taxa in top-N of <50% methods

**Why It's Needed:**
Categorizes taxa by robustness across methods. High agreement taxa are most reliable for clinical translation.

---

### **7.4 Method Agreement Heatmap**
**File:** `07_method_agreement_heatmap.png`  
**Type:** PNG Image (Heatmap)

**What It Shows:**
Pairwise agreement between MVA methods. Rows/columns = methods, cells = correlation or overlap percentage.

**How to Read:**
- Diagonal = 100% (method vs itself)
- Off-diagonal values show agreement
- Higher values (darker colors) = better agreement
- Overall high values = consistent findings across methods

**Why It's Needed:**
Assesses consistency across analytical approaches. High agreement increases confidence in findings.

---

### **7.5 Robustness Assessment Table**
**File:** `07_robustness_bootstrap_results.csv`

**Columns:**
```
Rank_Position, Most_Stable_Taxon, Stability_Percentage, Alternative_Taxa, Interpretation
```

**Example:**
```
1, Faecalibacterium_prausnitzii, 94%, Bifidobacterium(6%), highly_stable
2, Bifidobacterium_longum, 78%, Lactobacillus(15%)|Faecalibacterium(7%), moderately_stable
```

**How to Read:**
- Rank_Position = position in top-N ranking (1 = #1 most important)
- Most_Stable_Taxon = taxon appearing most often in this position across bootstrap samples
- Stability_Percentage = % of bootstrap samples where this taxon ranked in this position
- Alternative_Taxa = other taxa that sometimes ranked here (with frequencies)
- Interpretation:
  - ‚â•90%: highly_stable (very reliable)
  - 70-89%: moderately_stable (reliable)
  - 50-69%: somewhat_stable (caution)
  - <50%: unstable (not reliable)

**Why It's Needed:**
Quantifies ranking stability. Essential for understanding uncertainty in top biomarker identification. Unstable rankings suggest borderline features.

---

### **7.6 Robustness Violin Plot**
**File:** `07_robustness_violin_plot.png`  
**Type:** PNG Image (Violin plot)

**What It Shows:**
Distribution of rankings for top taxa across bootstrap samples. Each violin = one taxon, width = frequency at each rank.

**How to Read:**
- X-axis: Taxa names
- Y-axis: Rank position (1 = best)
- Violin width = how often taxon achieved each rank
- Narrow violins near top = stable ranking
- Wide violins = unstable ranking

**Why It's Needed:**
Visualizes ranking uncertainty. Complements stability table with full distribution view.

---

### **7.7 Top 10 Microbes Summary Card**
**File:** `07_top10_summary.pdf`  
**Type:** PDF Report (1-2 pages)

**What It Shows:**
Visual summary card with:
- Top 10 taxa names with consensus scores
- Evidence strength indicators (color-coded)
- Brief clinical interpretation for each
- Method agreement icons
- Bootstrap stability badges

**How to Read:**
- Designed for quick reference and communication
- Color coding: green = strong evidence, yellow = moderate, orange = weak
- Stars/badges indicate cross-method consistency

**Why It's Needed:**
Executive summary for stakeholders. Perfect for presentations, grant applications, or clinical team discussions.

---

## üîç **Category 8: Taxa Provenance Outputs**

### **8.1 Complete Taxa Provenance Table**
**File:** `08_taxa_provenance_complete.csv`

**Columns:**
```
Taxon_Name, Initial_Prevalence, Assigned_Groups, VarSel_Decision, VarSel_Reason, VarGroup_Composite, GroupSel_Decision, MVA_Rank_Cox, MVA_Rank_RF, Final_Status, Clinical_Interpretation
```

**How to Read:**
- One row per taxon in original dataset
- Traces complete journey through pipeline
- VarSel_Decision = included/excluded in variable selection
- VarGroup_Composite = which composite taxon joined (if any)
- Final_Status = included_in_final_model / excluded_at_[stage] / not_reached

**Why It's Needed:**
Complete audit trail for every taxon. Essential for transparency, debugging, and understanding why specific taxa were included/excluded.

---

### **8.2 Taxa Lifecycle Sankey Diagram**
**File:** `08_taxa_lifecycle_sankey.png`  
**Type:** PNG Image (Sankey/flow diagram)

**What It Shows:**
Flow diagram showing taxa counts through pipeline stages. Flows split at each stage showing included (green) vs excluded (red) paths.

**How to Read:**
- Left side: All taxa start (e.g., 350 taxa)
- Each stage shows split: taxa continuing (green) vs excluded (red)
- Flow width proportional to taxa count
- Right side: Final taxa in model

**Example Flow:**
```
350 taxa ‚Üí VarSel: 120 included (green), 230 excluded (red)
‚Üí 120 taxa ‚Üí VarGroup: merged into 8 composites
‚Üí 8 composites ‚Üí GroupSel: 3 selected (green), 5 excluded (red)
‚Üí 3 composites ‚Üí MVA: all 3 analyzed
```

**Why It's Needed:**
Visual overview of taxa attrition through pipeline. Helps identify which stages remove most taxa and validate selection stringency.

---

### **8.3 Exclusion Reasons Summary**
**File:** `08_exclusion_reasons_summary.csv`

**Columns:**
```
Stage, Exclusion_Reason, Taxa_Count, Example_Taxa
```

**Example:**
```
Variable_Selection, low_prevalence, 85, rare_taxon_1|rare_taxon_2|...
Variable_Selection, no_pfs_association, 145, escherichia_coli|...
Group_Selection, low_composite_importance, 5, Cluster_Rare_Taxa
```

**How to Read:**
- Stage = where exclusion occurred
- Exclusion_Reason = why taxa removed
- Taxa_Count = number of taxa excluded for this reason
- Example_Taxa = representative examples

**Why It's Needed:**
Summarizes exclusion decisions. Validates that appropriate criteria were used and not too stringent.

---

### **8.4 Individual Taxon Reports (Top 10)**
**File:** `08_taxon_report_[TaxonName].pdf`  
**Type:** PDF Report (one per top 10 taxon)

**What It Shows:**
Detailed report for each top-ranked taxon including:
- Initial abundance/prevalence stats
- Group assignments with confidence
- Selection decisions at each stage
- MVA results from all methods
- Clinical interpretation
- Literature references
- Biological mechanisms

**How to Read:**
Comprehensive profile for priority biomarkers. Use for in-depth understanding of individual taxa.

**Why It's Needed:**
Detailed documentation for priority biomarkers. Essential for manuscript preparation, grant applications, and validation study design.

---

## üìä **Category 9: Clinical Translation Outputs**

### **9.1 Risk Stratification Curves**
**File:** `09_risk_stratification_survival_curves.png`  
**Type:** PNG Image (Kaplan-Meier curves)

**What It Shows:**
PFS curves stratified by biomarker abundance tertiles (low/medium/high). Separate curves for top 3-5 biomarkers.

**How to Read:**
- X-axis: Time (months)
- Y-axis: Progression-free survival probability (0-1)
- Multiple lines: low/medium/high abundance groups
- P-value: log-rank test comparing groups
- Shaded regions: 95% confidence intervals

**Interpretation:**
- Diverging curves = biomarker associated with PFS
- Higher curves = better survival
- Non-overlapping confidence bands = statistically significant separation

**Why It's Needed:**
Shows clinical utility of biomarkers. Essential for demonstrating prognostic value and informing clinical decisions.

---

### **9.2 Clinical Impact Table**
**File:** `09_clinical_impact_summary.csv`

**Columns:**
```
Biomarker, Low_Group_Median_PFS, High_Group_Median_PFS, Difference_Months, HR_High_vs_Low, P_Value, Clinical_Significance
```

**Example:**
```
Faecalibacterium_prausnitzii, 12.3, 24.8, 12.5, 0.42, 0.001, High abundance associated with 12.5 month longer PFS
```

**How to Read:**
- Median_PFS = median progression-free survival in months
- Difference = clinical benefit (months)
- HR_High_vs_Low = hazard ratio comparing high vs low abundance groups
- Clinical_Significance = plain language interpretation

**Why It's Needed:**
Translates statistical results to clinical meaningful units (months of survival). Essential for clinician understanding and patient counseling.

---

### **9.3 Predictive Performance Summary**
**File:** `09_predictive_performance_table.csv`

**Columns:**
```
Model, Variables_Included, AUROC, Sensitivity, Specificity, PPV, NPV, Clinical_Utility_Score
```

**How to Read:**
- Sensitivity = % of relapsers correctly identified (true positive rate)
- Specificity = % of non-relapsers correctly identified (true negative rate)
- PPV = Positive Predictive Value (if test says relapse, probability it's correct)
- NPV = Negative Predictive Value (if test says no relapse, probability it's correct)
- Clinical_Utility_Score = weighted metric considering costs of false positives/negatives

**Why It's Needed:**
Evaluates clinical usefulness of predictive models. Different metrics matter for different applications (screening vs diagnosis).

---

### **9.4 Patient Risk Profile Template**
**File:** `09_patient_risk_profile_template.pdf`  
**Type:** PDF Template

**What It Shows:**
Template for individual patient reports including:
- Patient microbiome composition (top biomarkers)
- Risk score (0-100 scale)
- Risk category (low/medium/high)
- Contributing microbes with effect directions
- Clinical recommendations
- Confidence level

**How to Read:**
Designed for clinical use. Can be filled with individual patient data to generate personalized risk reports.

**Why It's Needed:**
Facilitates translation to clinical practice. Enables point-of-care decision support.

---

### **9.5 Clinical Recommendations Summary**
**File:** `09_clinical_recommendations.pdf`  
**Type:** PDF Report (2-3 pages)

**What It Shows:**
Evidence-based clinical recommendations including:
- Priority biomarkers for monitoring
- Suggested interventions (probiotics, dietary, FMT)
- Patient populations most likely to benefit
- Monitoring frequency
- Validation study design
- Implementation considerations

**How to Read:**
Structured recommendations organized by evidence strength (strong/moderate/weak). Each recommendation includes rationale and supporting data.

**Why It's Needed:**
Bridges research to practice. Provides actionable guidance for clinicians and informs future research priorities.

---

## üìÑ **Category 10: Summary Reports & Documentation**

### **10.1 Executive Summary Report**
**File:** `10_executive_summary.pdf`  
**Type:** PDF Report (1 page)

**Contents:**
- Study overview (N patients, N taxa analyzed)
- Key findings (top 3-5 biomarkers with effect sizes)
- Model performance (AUROC, C-index)
- Clinical implications (expected survival benefit)
- Next steps (validation, implementation)

**How to Read:**
Designed for busy stakeholders. Contains only essential information. No technical jargon.

**Why It's Needed:**
High-level communication for administrators, clinicians, and grant reviewers. Gets buy-in for further research.

---

### **10.2 Methods Compendium**
**File:** `10_methods_compendium.pdf`  
**Type:** PDF Report (10-15 pages)

**Contents:**
- Complete methodological documentation
- Data processing steps with parameters
- Statistical methods with justifications
- Software versions and packages
- Sensitivity analyses performed
- Limitations and assumptions
- Reproducibility information

**How to Read:**
Comprehensive technical documentation. Suitable for methods section of manuscript or detailed protocol.

**Why It's Needed:**
Full transparency and reproducibility. Essential for peer review, data sharing, and future replication.

---

### **10.3 Results Compendium (All Tables)**
**File:** `10_results_compendium.xlsx`  
**Type:** Excel Workbook (multiple sheets)

**Contents:**
- One sheet per major result table
- Formatted for publication (APA style)
- Includes supplementary tables
- Color coding for significance
- Embedded notes and legends

**How to Read:**
Organized by pipeline stage (sheets: DataCuration, MicrobialGrouping, VariableSelection, etc.). Each sheet = publication-ready table.

**Why It's Needed:**
Consolidated results for manuscript preparation. Easy to extract specific tables for publication or supplementary materials.

---

### **10.4 Pipeline Execution Log**
**File:** `10_pipeline_execution_log.txt`

**Format:**
```
PIPELINE EXECUTION LOG
Pipeline Version: 1.0.0
Execution Start: 2026-01-12 15:30:00
Execution End: 2026-01-12 17:00:00
Total Duration: 90 minutes

STAGE 1: DATA CURATION
Start: 15:30:00, End: 15:35:00, Duration: 5 min
Status: SUCCESS
Input: raw_counts.csv (435 taxa, 162 samples)
Output: normalized_data (350 taxa, 150 samples)
Parameters: {normalization: CLR, min_prevalence: 0.10}

STAGE 2: MICROBIAL GROUPING
Start: 15:35:00, End: 15:36:00, Duration: 1 min
Status: SUCCESS
Groups defined: 3 (SGroup1, SGroup2, SGroup3)
Taxa assigned: 350
...
```

**How to Read:**
Chronological execution log with timestamps, status, and parameters for each stage.

**Why It's Needed:**
Audit trail for pipeline execution. Essential for debugging, reproducibility, and documentation.

---

### **10.5 Data Provenance Report**
**File:** `10_data_provenance.json`  
**Type:** JSON file

**Contents:**
```json
{
  "input_files": [
    {"path": "raw_counts.csv", "md5": "abc123", "created": "2026-01-10"},
    {"path": "clinical.xlsx", "md5": "def456", "created": "2026-01-10"}
  ],
  "software_versions": {
    "python": "3.9.7",
    "lifelines": "0.27.0",
    "scikit-learn": "1.3.0"
  },
  "parameters": {
    "data_curation": {...},
    "variable_selection": {...},
    ...
  }
}
```

**How to Read:**
Machine-readable provenance information. Tracks all inputs, software versions, and parameters.

**Why It's Needed:**
Complete reproducibility. Enables exact replication of analysis. Required for data sharing and open science.

---

# üîÑ **LIST 2: Multi-Run Comparison Outputs**

These outputs are generated when comparing multiple pipeline runs (e.g., different variable selection methods, different grouping strategies) using the **same source data and microbial grouping**.

---

## üìä **Category 1: Performance Comparison Outputs**

### **C1.1 Cross-Run Performance Table**
**File:** `comparison_performance_summary.csv`

**Columns:**
```
Run_Name, Configuration, AUROC, C_Index, Brier_Score, AIC, N_Features, N_Significant, Best_Predictor, Best_P_Value
```

**Example:**
```
Run1_Prevalence_Only, VarSel:prevalence|MVA:cox, 0.75, 0.72, 0.19, 512.3, 120, 8, Faecalibacterium, 0.002
Run2_PFS_Univariate, VarSel:pfs_univariate|MVA:cox, 0.82, 0.79, 0.16, 498.6, 85, 12, Bifidobacterium, 0.001
Run3_Combined, VarSel:prevalence+pfs|MVA:cox, 0.84, 0.81, 0.15, 485.2, 65, 15, Faecalibacterium, 0.0008
```

**How to Read:**
- Each row = one pipeline configuration
- Compare AUROC/C_Index across runs (higher = better predictive ability)
- Compare AIC across runs (lower = better model fit)
- Compare N_Features (fewer features with good performance = more parsimonious)
- Configuration column shows key parameter differences

**Why It's Needed:**
**Most important comparison table.** Identifies which analytical strategy produces best performance. Essential for selecting optimal pipeline configuration.

---

### **C1.2 Performance Comparison Radar Plot**
**File:** `comparison_performance_radar.png`  
**Type:** PNG Image (Radar/spider plot)

**What It Shows:**
Multi-axis radar plot with each axis representing a performance metric (AUROC, Sensitivity, Specificity, Calibration, Parsimony). Different colored lines = different runs.

**How to Read:**
- Each vertex = one performance metric
- Larger polygons = better overall performance
- Compare polygon shapes and sizes
- Look for runs that dominate on most axes

**Why It's Needed:**
Visual comparison of multiple performance dimensions. Shows trade-offs (e.g., high AUROC but poor calibration) that aren't obvious in tables.

---

### **C1.3 ROC Curves Overlay**
**File:** `comparison_roc_curves_overlay.png`  
**Type:** PNG Image (Multiple ROC curves)

**What It Shows:**
ROC curves from all pipeline runs on same plot. Different colors/line styles = different runs.

**How to Read:**
- Curves further from diagonal = better discrimination
- Legend shows AUROC for each run
- Identify run with highest curve (best discrimination)
- Check if differences are visually meaningful

**Why It's Needed:**
Direct visual comparison of discriminative ability. Shows if performance differences are substantial or marginal.

---

### **C1.4 Calibration Curves Overlay**
**File:** `comparison_calibration_curves_overlay.png`  
**Type:** PNG Image (Multiple calibration curves)

**What It Shows:**
Calibration curves from all runs on same plot. Ideal calibration = diagonal line.

**How to Read:**
- Compare curve proximity to diagonal
- Curves above diagonal = overestimation of risk
- Curves below diagonal = underestimation of risk
- Identify run with best calibration (closest to diagonal)

**Why It's Needed:**
Assesses prediction accuracy across runs. Critical for clinical application where accurate probabilities needed.

---

### **C1.5 Statistical Significance Tests**
**File:** `comparison_statistical_tests.csv`

**Columns:**
```
Comparison, Metric, Run1_Value, Run2_Value, Difference, Test_Statistic, P_Value, Significant
```

**Example:**
```
Run1_vs_Run2, AUROC, 0.75, 0.82, 0.07, Z=2.34, 0.019, Yes
Run1_vs_Run3, AUROC, 0.75, 0.84, 0.09, Z=2.89, 0.004, Yes
Run2_vs_Run3, AUROC, 0.82, 0.84, 0.02, Z=0.78, 0.435, No
```

**How to Read:**
- Difference = magnitude of performance difference
- P_Value < 0.05 = statistically significant difference
- Significant = Yes means runs perform differently
- Significant = No means difference could be due to chance

**Why It's Needed:**
Determines if performance differences are real or due to random variation. Essential for claiming one method is "better" than another.

---

## üéØ **Category 2: Feature Comparison Outputs**

### **C2.1 Feature Selection Overlap Analysis**
**File:** `comparison_feature_overlap.csv`

**Columns:**
```
Run1, Run2, N_Run1_Features, N_Run2_Features, N_Overlap, Overlap_Percentage, Unique_to_Run1, Unique_to_Run2, Shared_Features
```

**How to Read:**
- Overlap_Percentage = consistency between runs (high = methods agree on important features)
- Unique lists show method-specific selections
- High overlap (>70%) = robust feature selection
- Low overlap (<50%) = method-dependent results (concerning)

**Why It's Needed:**
Assesses robustness of feature identification. High overlap across methods increases confidence in selected biomarkers.

---

### **C2.2 Feature Overlap Venn Diagram**
**File:** `comparison_feature_overlap_venn.png`  
**Type:** PNG Image (Venn diagram or UpSet plot)

**What It Shows:**
Visual representation of feature overlap across runs. For 2-3 runs: Venn diagram. For 4+ runs: UpSet plot.

**How to Read:**
- Overlapping regions = features selected by multiple runs
- Non-overlapping regions = run-specific features
- Central overlap = features selected by ALL runs (most robust)
- Size of regions = number of features

**Why It's Needed:**
Intuitive visual of feature consistency. Helps identify core biomarkers (selected by all methods) vs. method-specific findings.

---

### **C2.3 Feature Ranking Correlation Matrix**
**File:** `comparison_feature_ranking_correlation.csv`

**Columns:**
- Row/Column headers: Run names
- Cells: Spearman correlation of feature importance rankings

**Example:**
```
           Run1   Run2   Run3
Run1       1.00   0.72   0.68
Run2       0.72   1.00   0.85
Run3       0.68   0.85   1.00
```

**How to Read:**
- Diagonal = 1.00 (perfect correlation with self)
- Off-diagonal values = ranking agreement between runs
- High correlations (>0.70) = consistent feature importance rankings
- Low correlations (<0.50) = different runs prioritize different features

**Why It's Needed:**
Quantifies consistency of feature importance across methods. Supplements overlap analysis by considering ranking, not just selection.

---

### **C2.4 Feature Ranking Heatmap**
**File:** `comparison_feature_ranking_heatmap.png`  
**Type:** PNG Image (Heatmap)

**What It Shows:**
Heatmap with features as rows, runs as columns, cells colored by importance rank or score. Features sorted by consensus importance.

**How to Read:**
- Darker colors = higher importance
- Horizontal patterns (dark across row) = feature important in all runs (robust)
- Vertical patterns (dark in one column) = run-specific importance
- Top rows = highest consensus importance

**Why It's Needed:**
Visual identification of robust features. Easy to spot consistently important features vs. method-specific ones.

---

### **C2.5 Top 10 Biomarkers Consistency Table**
**File:** `comparison_top10_consistency.csv`

**Columns:**
```
Biomarker, N_Runs_In_Top10, Runs_Where_Top10, Mean_Rank, Std_Rank, Rank_Stability, Consensus_Interpretation
```

**Example:**
```
Faecalibacterium_prausnitzii, 3, Run1|Run2|Run3, 1.7, 0.5, highly_stable, Consistently top-ranked across all methods
Bifidobacterium_longum, 2, Run2|Run3, 4.5, 2.1, moderately_stable, Top biomarker in most methods
```

**How to Read:**
- N_Runs_In_Top10 = how many runs ranked this in top 10
- Mean_Rank = average rank across runs (lower = more important)
- Std_Rank = ranking variability (lower = more stable)
- Rank_Stability = interpretation of stability
- Biomarkers appearing in all runs = most robust

**Why It's Needed:**
Identifies biomarkers that are consistently important regardless of analytical choices. These are priority targets for validation.

---

## üìà **Category 3: Method Robustness Outputs**

### **C3.1 Robustness Summary Table**
**File:** `comparison_robustness_summary.csv`

**Columns:**
```
Run_Name, Bootstrap_Consistency, Cross_Validation_Stability, Feature_Selection_Reproducibility, Overall_Robustness_Score
```

**How to Read:**
- Bootstrap_Consistency = stability across bootstrap resamples (0-1, higher = more stable)
- CV_Stability = consistency across cross-validation folds
- Feature_Selection_Reproducibility = consistency of selected features across runs
- Overall_Robustness_Score = composite metric (0-100)

**Why It's Needed:**
Identifies which pipeline configuration produces most reproducible results. Robustness critical for clinical translation.

---

### **C3.2 Sensitivity Analysis Results**
**File:** `comparison_sensitivity_analysis.csv`

**Columns:**
```
Parameter_Varied, Parameter_Values, AUROC_Range, Feature_Overlap_Range, Interpretation
```

**Example:**
```
Prevalence_Threshold, 0.05|0.10|0.15|0.20, 0.80-0.84, 65-85%, Moderate sensitivity - results stable across reasonable range
PFS_P_Threshold, 0.05|0.10|0.20, 0.78-0.85, 45-90%, High sensitivity - threshold choice critical
```

**How to Read:**
- Parameter_Varied = which parameter was changed across runs
- AUROC_Range = spread of performance (narrow = robust to parameter choice)
- Feature_Overlap_Range = consistency of feature selection (wide range = parameter-sensitive)
- Interpretation = guidance on parameter importance

**Why It's Needed:**
Assesses sensitivity to parameter choices. Identifies which parameters critically affect results (need careful selection) vs. which are robust.

---

### **C3.3 Stability vs Performance Trade-off Plot**
**File:** `comparison_stability_vs_performance.png`  
**Type:** PNG Image (Scatter plot)

**What It Shows:**
Scatter plot with X-axis = stability metric, Y-axis = performance metric (AUROC). Each point = one run. Points in top-right quadrant = best (high stability + high performance).

**How to Read:**
- Top-right = ideal (stable + accurate)
- Top-left = accurate but unstable (may not replicate)
- Bottom-right = stable but inaccurate (consistently poor)
- Bottom-left = worst (unstable + inaccurate)

**Why It's Needed:**
Shows trade-off between performance and reproducibility. Sometimes highest-performing method is unstable (not ideal for clinical use).

---

## üß¨ **Category 4: Biological Coherence Outputs**

### **C4.1 Biological Pathway Consistency**
**File:** `comparison_pathway_consistency.csv`

**Columns:**
```
Pathway, N_Runs_Enriched, Enrichment_P_Values, Mean_Enrichment, Interpretation
```

**Example:**
```
Butyrate_Production, 3, 0.001|0.003|0.002, 4.5, Consistently enriched - strong biological signal
Propionate_Production, 2, 0.01|0.08|0.25, 2.1, Inconsistent - method-dependent
```

**How to Read:**
- N_Runs_Enriched = how many runs identified this pathway as enriched
- Enrichment_P_Values = significance across runs (all low = consistent signal)
- Mean_Enrichment = average fold-enrichment
- Pathways enriched in all runs = robust biological findings

**Why It's Needed:**
Validates that different analytical choices converge on same biological mechanisms. Increases confidence in biological interpretations.

---

### **C4.2 Functional Coherence Score**
**File:** `comparison_functional_coherence.csv`

**Columns:**
```
Run_Name, N_Features, N_Biological_Pathways, Features_Per_Pathway, Coherence_Score, Interpretation
```

**How to Read:**
- Coherence_Score = measure of biological relatedness among selected features
- High scores = selected features share functional pathways (biologically coherent)
- Low scores = selected features functionally diverse (may include noise)

**Why It's Needed:**
Assesses biological plausibility of feature sets. Coherent feature sets more likely to represent true biological signal.

---

## üìä **Category 5: Decision Support Outputs**

### **C5.1 Recommendation Matrix**
**File:** `comparison_recommendations_matrix.csv`

**Columns:**
```
Criterion, Weight, Run1_Score, Run2_Score, Run3_Score, Best_Run, Rationale
```

**Example:**
```
Predictive_Performance, 0.30, 0.75, 0.82, 0.84, Run3, Highest AUROC
Feature_Stability, 0.25, 0.88, 0.76, 0.72, Run1, Most reproducible features
Clinical_Interpretability, 0.20, 0.90, 0.85, 0.80, Run1, Fewest features, clearest interpretation
Biological_Coherence, 0.15, 0.82, 0.88, 0.85, Run2, Strongest pathway enrichment
Computational_Efficiency, 0.10, 0.95, 0.70, 0.65, Run1, Fastest execution
Overall_Weighted_Score, 1.00, 0.84, 0.81, 0.79, Run1, Best overall balance
```

**How to Read:**
- Each criterion scored 0-1 (higher = better)
- Weight reflects importance (sum to 1.0)
- Overall_Score = weighted sum
- Best_Run = highest overall score
- Rationale = justification for best choice on each criterion

**Why It's Needed:**
**Most important decision support tool.** Provides evidence-based recommendation for which pipeline configuration to use. Balances multiple criteria with explicit weights.

---

### **C5.2 Trade-off Analysis Summary**
**File:** `comparison_tradeoffs_summary.pdf`  
**Type:** PDF Report (2-3 pages)

**Contents:**
- Discussion of key trade-offs across runs
- Performance vs. Parsimony (accuracy vs. simplicity)
- Stability vs. Performance (reproducibility vs. accuracy)
- Interpretability vs. Predictive power
- Computational cost vs. Thoroughness
- Recommendation based on study goals

**How to Read:**
Narrative summary discussing advantages and disadvantages of each run. Helps researchers choose configuration aligned with their priorities.

**Why It's Needed:**
Provides context for decision-making. Different research goals prioritize different criteria (discovery vs. validation vs. clinical implementation).

---

### **C5.3 Scenario-Based Recommendations**
**File:** `comparison_scenario_recommendations.csv`

**Columns:**
```
Scenario, Priority_Criteria, Recommended_Run, Rationale
```

**Example:**
```
Discovery_Study, predictive_performance|comprehensive, Run3, Maximize discovery of potential biomarkers
Validation_Study, stability|reproducibility, Run1, Ensure reproducible findings for validation
Clinical_Implementation, interpretability|parsimony, Run1, Simple model for clinical adoption
Grant_Application, performance|novelty, Run3, Highest performance to demonstrate feasibility
```

**How to Read:**
- Match your study goal to scenario
- Use recommended run for that scenario
- Rationale explains why this run suits this goal

**Why It's Needed:**
Acknowledges that "best" method depends on context. Provides tailored recommendations for different research phases.

---

## üìÑ **Category 6: Consolidated Comparison Reports**

### **C6.1 Comparison Executive Summary**
**File:** `comparison_executive_summary.pdf`  
**Type:** PDF Report (1-2 pages)

**Contents:**
- Runs compared (configurations)
- Key performance differences (table)
- Feature consistency analysis (Venn diagram)
- Recommended configuration with justification
- Next steps

**How to Read:**
High-level summary for stakeholders. No technical jargon. Focus on "which method is best and why?"

**Why It's Needed:**
Communicates comparison results to non-technical audiences. Essential for getting buy-in on methodological choices.

---

### **C6.2 Detailed Comparison Report**
**File:** `comparison_detailed_report.pdf`  
**Type:** PDF Report (10-15 pages)

**Contents:**
- Complete methodological differences between runs
- Performance comparison (all metrics)
- Feature comparison (overlap, consistency)
- Robustness assessment (stability, sensitivity)
- Biological coherence analysis
- Statistical significance tests
- Recommendations with evidence

**How to Read:**
Comprehensive technical comparison. Suitable for methods section of manuscript or detailed protocol.

**Why It's Needed:**
Complete documentation of method comparison. Essential for transparent reporting and peer review.

---

### **C6.3 Side-by-Side Comparison Dashboard**
**File:** `comparison_dashboard.html`  
**Type:** HTML Interactive Dashboard

**What It Shows:**
Interactive web page with:
- Performance metrics comparison (bar charts)
- ROC/calibration curves (interactive)
- Feature overlap (interactive Venn/UpSet)
- Feature importance heatmaps
- Filters to customize view

**How to Read:**
Web browser-based interactive exploration. Hover over elements for details. Toggle between metrics and views.

**Why It's Needed:**
Enables detailed exploration without programming. Perfect for team discussions and presentations.

---

### **C6.4 Comparison Methods Table (Publication-Ready)**
**File:** `comparison_methods_table_publication.csv`

**Columns:**
```
Run, Variable_Selection, Grouping, MVA_Method, N_Features, AUROC, C_Index, Top_Biomarker
```

**Format:**
Publication-ready table following journal standards (APA style, formatted for import to manuscript).

**Why It's Needed:**
Ready for direct inclusion in manuscript. Saves formatting time and ensures consistency.

---

### **C6.5 Comparison Figure Panel (Publication-Ready)**
**File:** `comparison_figure_panel_publication.png`  
**Type:** PNG Image (Multi-panel figure)

**What It Shows:**
Combined figure with 4-6 panels:
- Panel A: Performance comparison (bar chart)
- Panel B: ROC curves overlay
- Panel C: Feature overlap (Venn)
- Panel D: Feature importance heatmap
- Panel E: Stability vs. performance (scatter)
- Panel F: Calibration curves

**How to Read:**
High-quality multi-panel figure ready for journal submission. Each panel tells part of the comparison story.

**Why It's Needed:**
Publication-ready figure. Journals often prefer multi-panel figures that tell complete story.

---

## üìä **Category 7: Meta-Analysis Outputs (3+ Runs)**

### **C7.1 Method Clustering Dendrogram**
**File:** `comparison_method_clustering.png`  
**Type:** PNG Image (Dendrogram)

**What It Shows:**
Hierarchical clustering of runs based on results similarity. Runs producing similar results cluster together.

**How to Read:**
- X-axis: Run names
- Y-axis: Dissimilarity (height of joins)
- Runs joined at low heights = very similar results
- Runs joined at high heights = different results
- Identify clusters of similar methods

**Why It's Needed:**
Identifies families of related methods. Helps understand which methodological choices have biggest impact on results.

---

### **C7.2 Consensus Across Methods**
**File:** `comparison_consensus_across_methods.csv`

**Columns:**
```
Biomarker, N_Runs, Frequency, Mean_Rank, Std_Rank, Evidence_Strength, Recommendation
```

**How to Read:**
- N_Runs = how many runs identified this biomarker as important
- Frequency = % of runs (100% = all runs)
- Evidence_Strength based on frequency:
  - High: ‚â•80% of runs
  - Moderate: 60-79% of runs
  - Low: 40-59% of runs
  - Insufficient: <40% of runs
- Recommendation = priority for follow-up

**Why It's Needed:**
Identifies biomarkers robust across methodological choices. These are highest priority for validation.

---

### **C7.3 Method Performance Distribution**
**File:** `comparison_performance_distribution.png`  
**Type:** PNG Image (Box plots / violin plots)

**What It Shows:**
Distribution of performance metrics across all runs. Separate box/violin for each metric (AUROC, C-index, etc.).

**How to Read:**
- Box shows quartiles (median + IQR)
- Whiskers show range
- Outliers shown as points
- Wide distributions = high method sensitivity
- Narrow distributions = robust across methods

**Why It's Needed:**
Shows spread of results across methods. Narrow distributions = results robust to methodological choices (good).

---

## üéØ **Summary**

### **Single Run Outputs (LIST 1): 70+ outputs**
- **10 Categories** covering entire pipeline from data QC to clinical translation
- **Essential for:** Documenting single analysis, manuscript preparation, clinical communication
- **Priority:** Categories 6-7 (MVA Methods, Consensus Analysis) most critical for biomarker identification

### **Multi-Run Comparison Outputs (LIST 2): 30+ outputs**
- **7 Categories** focused on comparing different analytical strategies
- **Essential for:** Method selection, robustness assessment, sensitivity analysis
- **Priority:** Categories 1, 2, 5 (Performance, Features, Decision Support) most critical for choosing best approach

### **Key Principles:**
1. **Transparency:** Every decision documented and traceable
2. **Interpretability:** Plain language explanations alongside technical metrics
3. **Clinical Relevance:** Results translated to clinically meaningful units
4. **Reproducibility:** Complete provenance and parameter documentation
5. **Quality:** Publication-ready formats following journal standards

**These outputs provide complete documentation for rigorous, reproducible, clinically translatable microbiome-PFS analysis in Multiple Myeloma!** üî¨üìäüè•

-------------------------------------------------------------------------------

Great question! Let me analyze whether all the specified outputs can be generated from the PipelineData structure and comparison framework we designed.
‚úÖ YES - Almost All Outputs Can Be Generated!
Here's the breakdown by category:
üìä LIST 1: Single Pipeline Run Outputs
Category 1: Data Quality & Curation ‚úÖ 100% Possible
Data Source: results["data_curation"], metadata, provenance
All outputs can be generated:
Quality report: From results["data_curation"]["quality_metrics"]
Normalization summary: From results["data_curation"]["normalization_info"]
Sample metadata: From data["clinical"] + metadata["sample_info"]
Category 2: Microbial Grouping ‚úÖ 100% Possible
Data Source: results["microbial_grouping"]
All outputs directly available:
Group definitions: From results["microbial_grouping"]["group_definitions"]
Taxa assignments: From results["microbial_grouping"]["group_assignments"]
Venn diagrams: Calculate from group_assignments
Abundance summary: From results["microbial_grouping"]["taxa_coverage"]
Category 3: Variable Selection ‚úÖ 100% Possible
Data Source: results["variable_selection"]
All outputs directly available:
Selection results: From results["variable_selection"]["selection_results"]
Method comparison: From results["variable_selection"]["method_comparison"]
Heatmaps/plots: Calculate from selection_results
Exclusion reasons: Extract from selection_results
Final features: From results["variable_selection"]["selected_features"]
Category 4: Variable Grouping ‚úÖ 100% Possible
Data Source: results["variable_grouping"]
All outputs directly available:
Composite features table: From results["variable_grouping"]["composite_features"]
Radar plots: From composite_features[*]["individual_contributions"]
Correlation matrices: From composite_features[*]["correlation_matrix"]
Variance explained: From results["variable_grouping"]["variance_explained"]
Category 5: Group Selection ‚úÖ 100% Possible
Data Source: results["group_selection"]
All outputs directly available:
Selection results: From results["group_selection"]["selection_results"]
Importance barplot: From results["group_selection"]["importance_scores"]
Final features: From results["group_selection"]["final_features"]
Category 6: MVA Methods ‚úÖ 100% Possible
Data Source: results["mva_methods"]
All outputs directly available:
Cox results table: From results["mva_methods"]["feature_importance"]["cox_regression"]
Forest plots: From hazard ratios + confidence intervals in model results
P-value plots: From p-values in model results
RF importance: From results["mva_methods"]["feature_importance"]["random_forest"]
Performance comparison: From results["mva_methods"]["performance_metrics"]
ROC curves: From results["mva_methods"]["model_objects"] (need to compute)
Calibration: From results["mva_methods"]["model_objects"] + data["clinical"]
Feature importance heatmap: From results["mva_methods"]["feature_importance"]
Note: ROC and calibration curves require accessing model objects and making predictions, but models are stored in the structure.
Category 7: Consensus Analysis ‚úÖ 100% Possible
Data Source: results["consensus_analysis"]
All outputs directly available:
Consensus ranking table: From results["consensus_analysis"]["standardized_scores"]
Consensus barplot: From results["consensus_analysis"]["top_n_rankings"]
Method agreement: From results["consensus_analysis"]["method_agreement"]
Robustness table: From results["consensus_analysis"]["robustness_metrics"]
Violin plots: Calculate from bootstrap data in robustness metrics
Top 10 summary: From results["consensus_analysis"]["top_n_rankings"]["ranking_details"]
Category 8: Taxa Provenance ‚úÖ 100% Possible
Data Source: taxa_provenance
All outputs directly available:
Complete provenance table: From taxa_provenance[*] (all taxa)
Sankey diagram: Calculate flows from taxa_provenance[*]["variable_selection"]["final_decision"], etc.
Exclusion reasons: Extract from taxa_provenance[*][stage]["exclusion_reason"]
Individual taxon reports: From taxa_provenance[taxon_name] (complete record)
Category 9: Clinical Translation ‚ö†Ô∏è 95% Possible (minor calculations needed)
Data Source: results["mva_methods"]["model_objects"] + data["clinical"]
Most outputs possible, some require predictions:
‚úÖ Risk stratification curves: Use model to predict on subgroups (LOW/MEDIUM/HIGH abundance tertiles)
Requires: Model predictions + stratification by biomarker levels
‚úÖ Clinical impact table: Calculate from Cox hazard ratios
Directly available from model results
‚úÖ Predictive performance: From results["mva_methods"]["performance_metrics"]
Directly available
‚úÖ Patient risk profiles: Use model to predict individual risks
Requires: Model predictions on individual patients
‚úÖ Clinical recommendations: Synthesize from all results
Can be generated from consensus rankings + hazard ratios
Minor gap: Need to run predictions using stored model objects to generate individual risk scores and stratification curves. But models are available in the structure.
Category 10: Summary Reports ‚úÖ 100% Possible
Data Source: All sections
All outputs directly available by aggregating:
Executive summary: Aggregate key findings from all sections
Methods compendium: From provenance["parameters_used"] + config
Results compendium: Extract all result tables
Execution log: From provenance["execution_log"]
Data provenance: From provenance section
üîÑ LIST 2: Multi-Run Comparison Outputs
All comparison outputs can be generated by comparing multiple PipelineData objects:
‚úÖ 100% Possible for All 7 Categories:
Performance Comparison - Compare results["mva_methods"]["performance_metrics"] across runs
Feature Comparison - Compare results["variable_selection"]["selected_features"] across runs
Method Robustness - Compare stability metrics across runs
Biological Coherence - Compare pathway enrichments across runs (if calculated)
Decision Support - Calculate recommendation scores from performance + stability metrics
Consolidated Reports - Aggregate comparisons
Meta-Analysis - Statistical analysis across multiple runs
üéØ Summary: Completeness Assessment
Fully Possible (No Additional Data Needed):
90-95% of outputs can be generated directly from the PipelineData structure
All tables, most plots, all documentation
Requires Minor Calculations (Data Available in Structure):
5-10% of outputs require using stored model objects to make predictions:
Risk stratification curves (need to predict on tertiles)
Individual patient risk scores (need to predict on individuals)
ROC curves (need predicted probabilities)
Calibration plots (need predicted vs. observed)
But these are NOT gaps in the data structure - the model objects are stored in results["mva_methods"]["model_objects"], so predictions can be made. Just requires one extra step: model.predict(data).
Potential Enhancements:
If you want 100% direct generation without any predictions, you could add to the structure:
"results": {    "mva_methods": {        # ... existing ...        "predictions": {            "cox_regression": {                "individual_predictions": pd.Series,  # Risk score per patient                "predicted_probabilities": pd.Series, # Probability per patient                "risk_tertiles": {                    "low": [...],    # Patient IDs                    "medium": [...],                    "high": [...]                }            }        }    }}
But this is optional since models are already stored and can generate predictions on demand.
‚ú® Final Answer:
YES, all ~100 specified outputs CAN be generated from:
‚úÖ Single PipelineData structure (for single run outputs)
‚úÖ Multiple PipelineData objects (for comparison outputs)
‚úÖ Stored model objects (for prediction-based outputs)
The data structure is complete! üéâ
The only "work" needed is:
Visualization code to convert data ‚Üí plots (standard matplotlib/seaborn)
Report generation code to format data ‚Üí PDFs/HTML (standard libraries)
Minor predictions using stored models (standard scikit-learn/lifelines methods)
No additional data collection or pipeline re-runs needed - everything is captured in the structure! üî¨üìä‚úÖ