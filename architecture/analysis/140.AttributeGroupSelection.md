# 140.AttributeGroupSelection.md: Group Selection/Discard Methods

This document presents **group selection and discard methods** to refine grouped variables, keeping only the most relevant composite features for final analysis while removing redundant or irrelevant groups.

---

## ðŸŽ¯ Phase 4 Overview

**Goal:** Reduce grouped features (potentially 10-15 groups) to the most informative composites (3-8 groups) that best represent the underlying microbial patterns.

**Why After Variable Grouping:** Start with biologically meaningful composites, then select the most clinically relevant ones for statistical modeling.

---

## ðŸ“Š Group Selection/Discard Methods

### **1. Univariate Group Screening**
**Description:** Test each grouped feature individually against PFS to identify groups with significant associations.

**Algorithm:**
```python
significant_groups = []

for group_feature in grouped_features:
    # Fit univariate model: PFS ~ group_abundance
    cph = CoxPHFitter()
    cph.fit(data[['time', 'event', group_feature]], duration_col='time', event_col='event')

    if cph.p_values_[group_feature] < significance_threshold:
        significant_groups.append(group_feature)

print(f"Selected {len(significant_groups)} significant groups from {len(grouped_features)} total")
```

**Parameters:**
- Significance threshold (default: p < 0.05 or p < 0.1 for liberal screening)
- Statistical test (Cox regression, log-rank test)
- Multiple testing correction (Bonferroni, FDR)

**Pros:**
- âœ… **Clinically focused** - Directly identifies PFS-relevant microbial groups
- âœ… **Statistical rigor** - Formal hypothesis testing for each group
- âœ… **Computational efficiency** - Fast individual group tests
- âœ… **Easy interpretation** - Clear inclusion criteria based on p-values
- âœ… **Biological insight** - Reveals which microbial patterns matter for outcomes

**Cons:**
- âŒ **Ignores group interactions** - May miss groups significant only in combination
- âŒ **Multiple testing issues** - Risk of false positives without correction
- âŒ **Conservative selection** - May exclude groups with weak individual effects
- âŒ **Context independence** - Doesn't account for clinical covariates
- âŒ **Sample size dependence** - Power varies with number of PFS events

**Limitations:**
- Requires sufficient PFS events for reliable statistical testing
- May miss synergistic effects between microbial groups
- Results sensitive to censoring patterns in survival data

**Why Choose:** Essential first step for clinically focused group selection, ensuring all retained groups have demonstrated PFS relevance.

**Expected Results:** Retains 40-70% of groups showing individual PFS associations

---

### **2. Multivariate Group Selection**
**Description:** Test groups in comprehensive models including clinical variables, selecting those significant in full clinical context.

**Algorithm:**
```python
# Fit full multivariate model
all_predictors = clinical_variables + grouped_features
cph_full = CoxPHFitter(penalizer=0.1)  # Add regularization
cph_full.fit(data[['time', 'event'] + all_predictors], duration_col='time', event_col='event')

# Select groups significant in multivariate context
group_p_values = cph_full.p_values_[grouped_features]
significant_groups = group_p_values[group_p_values < significance_threshold].index.tolist()

# Optional: Iterative refinement
while len(significant_groups) < desired_n_groups and any_non_significant:
    # Remove least significant group and refit
    # Continue until desired number of groups or convergence
```

**Parameters:**
- Significance threshold (default: p < 0.05)
- Clinical covariates to include (age, ISS, treatment variables)
- Regularization strength (default: 0.1 for stability)
- Iterative refinement criteria

**Pros:**
- âœ… **Clinically comprehensive** - Considers full clinical context
- âœ… **Context-aware selection** - Identifies groups significant beyond clinical factors
- âœ… **Multivariate validity** - Accounts for group intercorrelations and clinical confounding
- âœ… **Clinical translation** - Results directly relevant for patient risk stratification
- âœ… **Confounding control** - Adjusts for known clinical predictors of PFS

**Cons:**
- âŒ **Computational intensity** - Requires fitting complex multivariate models
- âŒ **Parameter instability** - Large models can be numerically unstable
- âŒ **Clinical variable dependence** - Results depend on which covariates are included
- âŒ **Overfitting risk** - Too many groups relative to sample size
- âŒ **Interpretation complexity** - Hard to attribute effects to individual groups

**Limitations:**
- Requires sufficient sample size for stable multivariate estimates
- Sensitive to multicollinearity between groups and clinical variables
- Clinical covariate selection significantly affects which groups appear significant
- Model convergence issues with high-dimensional group sets

**Why Choose:** Most clinically relevant approach for identifying microbial groups independently associated with PFS in real-world clinical context.

**Expected Results:** Retains 20-50% of groups significant in multivariate clinical models

---

### **3. Group Stability Selection**
**Description:** Use bootstrap resampling to identify groups with consistently significant PFS associations across multiple subsamples.

**Algorithm:**
```python
group_stability = {}
n_bootstraps = 200

for group_feature in grouped_features:
    significant_count = 0

    for _ in range(n_bootstraps):
        # Bootstrap sample
        bootstrap_indices = np.random.choice(len(data), size=len(data), replace=True)
        bootstrap_data = data.iloc[bootstrap_indices]

        # Test PFS association
        cph_bootstrap = CoxPHFitter()
        cph_bootstrap.fit(bootstrap_data[['time', 'event', group_feature]],
                         duration_col='time', event_col='event')

        if cph_bootstrap.p_values_[group_feature] < 0.05:
            significant_count += 1

    group_stability[group_feature] = significant_count / n_bootstraps

# Select stable groups
stable_groups = [group for group, stability in group_stability.items()
                if stability >= stability_threshold]
```

**Parameters:**
- Number of bootstrap samples (default: 100-500)
- Stability threshold (default: 0.7 = significant in 70% of bootstraps)
- Bootstrap sample size (default: 80% of original sample size)
- Significance threshold for each bootstrap (default: p < 0.05)

**Pros:**
- âœ… **Robust identification** - Finds consistently PFS-associated groups
- âœ… **Uncertainty quantification** - Provides confidence measures for selections
- âœ… **Cross-validation built-in** - Bootstrap validation of group associations
- âœ… **Sample variability control** - Accounts for population heterogeneity
- âœ… **Reproducibility assessment** - Identifies stable vs unstable associations

**Cons:**
- âŒ **Computationally expensive** - Requires many model fits per group
- âŒ **Time-intensive** - May take significant time for many groups
- âŒ **Parameter dependence** - Stability threshold affects final selections
- âŒ **Conservative bias** - May miss groups with moderate but real associations
- âŒ **Memory intensive** - Stores many bootstrap model objects

**Limitations:**
- Requires sufficient sample size for meaningful bootstrap distributions
- Assumes bootstrap samples represent population characteristics
- May be overly conservative for small datasets
- Computational cost scales with number of groups and bootstrap samples

**Why Choose:** Essential for reproducible biomarker discovery, ensuring selected groups have robust PFS associations across different patient subsets.

**Expected Results:** Identifies 3-8 groups with high stability scores (70%+ consistency across bootstraps)

---

### **4. Information-Theoretic Group Selection**
**Description:** Select groups based on mutual information with PFS outcomes, capturing non-linear and complex associations.

**Algorithm:**
```python
group_mi_scores = {}

for group_feature in grouped_features:
    # Calculate mutual information I(group_abundance; PFS_outcome)
    mi_score = mutual_info_regression(data[[group_feature]], data['pfs_outcome'])[0]

    # Compare to null distribution (permuted PFS)
    n_permutations = 1000
    permuted_mi_scores = []
    for _ in range(n_permutations):
        permuted_pfs = np.random.permutation(data['pfs_outcome'])
        permuted_mi = mutual_info_regression(data[[group_feature]], permuted_pfs)[0]
        permuted_mi_scores.append(permuted_mi)

    # Calculate empirical p-value
    p_value = np.mean(np.array(permuted_mi_scores) >= mi_score)
    group_mi_scores[group_feature] = {'mi_score': mi_score, 'p_value': p_value}

# Select groups with significant MI
significant_groups = [group for group, stats in group_mi_scores.items()
                     if stats['p_value'] < significance_threshold]
```

**Parameters:**
- Mutual information estimator (default: k-nearest neighbors)
- Number of permutations for significance testing (default: 1000)
- Significance threshold (default: p < 0.05 after correction)
- k parameter for k-NN estimator (default: 3)

**Pros:**
- âœ… **Non-linear relationships** - Captures complex group-PFS associations
- âœ… **No distribution assumptions** - Works with any abundance distribution
- âœ… **Model independence** - Doesn't assume specific relationship form
- âœ… **Robust to outliers** - Less sensitive to extreme group abundance values
- âœ… **Information completeness** - Measures total dependence between variables

**Cons:**
- âŒ **Computational cost** - Especially intensive for permutation testing
- âŒ **Estimator sensitivity** - Results depend on k parameter and estimator choice
- âŒ **Limited directionality** - MI doesn't indicate relationship direction or strength
- âŒ **Multiple testing** - Requires correction for many groups
- âŒ **Interpretability challenges** - MI scores don't provide mechanistic insight

**Limitations:**
- Requires sufficient sample size for reliable MI estimation
- Sensitive to estimator parameters and discretization choices
- Doesn't provide confidence intervals or effect size measures
- May select redundant groups with similar information content

**Why Choose:** Excellent for discovering non-linear microbial group-PFS relationships that parametric methods might miss.

**Expected Results:** Selects 3-7 groups with significant mutual information with PFS

---

### **5. Boruta Group Selection**
**Description:** Use ensemble feature selection to identify all groups with predictive relevance for PFS, not just the strongest ones.

**Algorithm:**
```python
# Create shadow features (randomized copies of real groups)
shadow_features = create_shadow_features(grouped_features, n_shadows=5)

# Combine real and shadow features
all_features = grouped_features + shadow_features

# Train random survival forest
rsf = RandomSurvivalForest(n_estimators=1000)
rsf.fit(data[all_features], data[['time', 'event']])

# Compare real vs shadow importance
feature_importance = rsf.feature_importances_

# Iteratively remove features less important than best shadow
while any_real_less_than_best_shadow(feature_importance):
    # Remove least important real features below shadow threshold
    # Retrain and repeat
    pass

selected_groups = remaining_real_features()
```

**Parameters:**
- Number of shadow features per real group (default: 3-5)
- Random survival forest parameters (n_estimators=1000, max_depth=None)
- Stopping criteria (max iterations or stability threshold)
- Importance measure (default: permutation importance)

**Pros:**
- âœ… **All-relevant selection** - Finds all PFS-predictive groups, not just top performers
- âœ… **Statistical foundation** - Uses permutation testing for significance
- âœ… **Handles group correlations** - Works well with correlated microbial composites
- âœ… **Ensemble robustness** - Reduces variance through multiple tree averaging
- âœ… **No parameter tuning** - Algorithm determines optimal group set

**Cons:**
- âŒ **Computationally intensive** - Multiple random forest trainings required
- âŒ **Time-consuming** - May take hours for many groups and trees
- âŒ **Memory intensive** - Large forest objects for ensemble methods
- âŒ **Random forest dependence** - Results depend on survival forest implementation
- âŒ **May be overly inclusive** - Includes marginally relevant groups

**Limitations:**
- Requires sufficient sample size for stable random forest importance measures
- May be conservative in small datasets with few PFS events
- Computational requirements may be prohibitive for very large group sets
- Importance measures can be unstable across different random seeds

**Why Choose:** Comprehensive approach ensuring no clinically relevant microbial groups are missed in PFS prediction.

**Expected Results:** Selects 4-10 groups with confirmed predictive relevance for PFS

---

### **6. Elastic Net Group Selection**
**Description:** Use regularized regression to automatically select groups with PFS predictive value through coefficient shrinkage.

**Algorithm:**
```python
# Prepare data
X = data[grouped_features]  # Group abundances
y = data[['time', 'event']]  # PFS outcomes

# Fit elastic net with survival adaptation
# (Using glmnet or similar with survival loss function)
enet_model = ElasticNetSurvival(alpha=0.1, l1_ratio=0.5)
enet_model.fit(X, y)

# Select groups with non-zero coefficients
selected_groups = grouped_features[enet_model.coef_ != 0]

# Optional: Cross-validation for parameter selection
cv_model = ElasticNetSurvivalCV(l1_ratios=[0.1, 0.5, 0.9], alphas=np.logspace(-4, 0, 50))
cv_model.fit(X, y)
final_selected_groups = grouped_features[cv_model.coef_ != 0]
```

**Parameters:**
- L1 ratio (balance between L1 and L2 regularization, default: 0.5)
- Regularization strength Î± (chosen by cross-validation)
- Maximum iterations (default: 1000)
- Convergence tolerance (default: 1e-4)

**Pros:**
- âœ… **Automated selection** - No manual threshold setting required
- âœ… **Correlation handling** - L2 component manages multicollinear groups
- âœ… **Continuous selection** - Gradual elimination rather than hard cutoffs
- âœ… **Cross-validation built-in** - Automatic parameter optimization
- âœ… **Predictive focus** - Selects for PFS prediction performance

**Cons:**
- âŒ **Linear assumptions** - Assumes linear relationships for group selection
- âŒ **Parameter sensitivity** - Regularization balance affects selected groups
- âŒ **Survival adaptation** - Requires specialized elastic net for survival data
- âŒ **May miss weak signals** - Conservative selection approach
- âŒ **Computational cost** - Cross-validation increases processing time

**Limitations:**
- Requires survival-specific elastic net implementation
- Selection depends on regularization parameter choice
- May not capture non-linear group-PFS relationships
- Cross-validation can be computationally expensive

**Why Choose:** Provides statistically principled, automated group selection with built-in correlation handling for PFS prediction.

**Expected Results:** Selects 3-8 groups with non-zero coefficients in regularized survival model

---

### **7. Group Correlation Analysis**
**Description:** Select groups based on their correlation patterns, keeping diverse, non-redundant groups that capture different microbial aspects.

**Algorithm:**
```python
# Calculate correlation matrix between groups
group_corr_matrix = data[grouped_features].corr().abs()

# Apply hierarchical clustering to identify redundant groups
from scipy.cluster.hierarchy import linkage, fcluster
linkage_matrix = linkage(squareform(1 - group_corr_matrix), method='complete')
cluster_labels = fcluster(linkage_matrix, t=correlation_threshold, criterion='distance')

# Select one representative group from each cluster
selected_groups = []
for cluster_id in np.unique(cluster_labels):
    cluster_groups = [grouped_features[i] for i in range(len(grouped_features))
                     if cluster_labels[i] == cluster_id]

    # Choose representative (highest variance or clinical relevance)
    representative = select_representative(cluster_groups, data)
    selected_groups.append(representative)
```

**Parameters:**
- Correlation threshold for clustering (default: 0.8)
- Representative selection criterion (variance, univariate PFS association, etc.)
- Clustering method (complete, average, ward linkage)

**Pros:**
- âœ… **Redundancy reduction** - Eliminates highly correlated, redundant groups
- âœ… **Diversity preservation** - Keeps groups representing different microbial patterns
- âœ… **Stability enhancement** - Less sensitive to individual group variations
- âœ… **Biological coverage** - Ensures different microbial aspects are represented
- âœ… **Multivariate insight** - Reveals group relationships and dependencies

**Cons:**
- âŒ **Arbitrary thresholds** - Correlation cutoff affects final selections
- âŒ **Representative bias** - Choice of representative affects retained information
- âŒ **Information loss** - Discards potentially relevant but correlated groups
- âŒ **Clustering dependence** - Results vary with clustering method and parameters
- âŒ **Context ignorance** - Doesn't consider clinical relevance in correlation analysis

**Limitations:**
- May discard biologically important but correlated groups
- Representative selection criteria are subjective
- Doesn't account for non-linear group relationships
- Correlation patterns may vary across patient subgroups

**Why Choose:** Ensures selected groups are diverse and non-redundant, providing comprehensive microbial coverage for PFS analysis.

**Expected Results:** Selects 4-8 diverse groups representing different microbial patterns

---

### **8. Clinical Relevance Group Selection**
**Description:** Select groups based on their relationships with specific clinical variables relevant to Multiple Myeloma.

**Algorithm:**
```python
clinical_variables = ['age', 'ISS_stage', 'treatment_duration', 'complications']
group_clinical_relevance = {}

for group_feature in grouped_features:
    clinical_associations = {}

    for clinical_var in clinical_variables:
        # Test association between group and clinical variable
        if clinical_var in ['age', 'treatment_duration']:  # Continuous
            correlation, p_value = pearsonr(data[group_feature], data[clinical_var])
        else:  # Categorical
            statistic, p_value = kruskal_wallis_test(data, group_feature, clinical_var)

        clinical_associations[clinical_var] = {'correlation': correlation, 'p_value': p_value}

    # Calculate overall clinical relevance score
    relevance_score = calculate_relevance_score(clinical_associations)
    group_clinical_relevance[group_feature] = relevance_score

# Select most clinically relevant groups
selected_groups = sorted(group_clinical_relevance.items(), key=lambda x: x[1], reverse=True)[:n_groups]
selected_groups = [group for group, score in selected_groups]
```

**Parameters:**
- Clinical variables to consider (MM-specific: age, ISS, treatment, complications)
- Association test for each variable type (correlation for continuous, Kruskal-Wallis for categorical)
- Relevance scoring method (weighted sum, maximum association, etc.)
- Number of groups to select

**Pros:**
- âœ… **Disease-specific** - Focuses on MM-relevant clinical associations
- âœ… **Biological insight** - Links microbial groups to clinical phenotypes
- âœ… **Clinical translation** - Identifies groups relevant for patient management
- âœ… **Multifactorial consideration** - Accounts for multiple clinical aspects
- âœ… **Mechanistic clues** - Suggests how microbial groups influence clinical outcomes

**Cons:**
- âŒ **Clinical variable selection** - Depends on which variables are included
- âŒ **Association strength** - May prioritize groups associated with common rather than important variables
- âŒ **Confounding issues** - Clinical associations may be indirect or confounded
- âŒ **Multiple testing** - Many clinical associations to evaluate
- âŒ **Interpretation complexity** - Hard to determine which associations are most meaningful

**Limitations:**
- Requires comprehensive clinical data for all patients
- Clinical variable selection significantly affects results
- May miss groups with subtle but important clinical associations
- Association strength doesn't prove causation

**Why Choose:** Ensures selected groups have demonstrated clinical relevance beyond just PFS associations.

**Expected Results:** Selects 3-6 groups with strongest associations to key MM clinical variables

---

### **9. Consensus Group Selection**
**Description:** Apply multiple selection methods and retain groups selected by consensus across approaches.

**Algorithm:**
```python
# Apply multiple selection methods
univariate_selected = univariate_group_screening(grouped_features, data)
multivariate_selected = multivariate_group_selection(grouped_features, data, clinical_vars)
stability_selected = stability_group_selection(grouped_features, data)

# Find consensus
all_selected_groups = [univariate_selected, multivariate_selected, stability_selected]

# Method 1: Intersection (groups selected by ALL methods)
consensus_strict = set.intersection(*[set(groups) for groups in all_selected_groups])

# Method 2: Union with voting (groups selected by multiple methods)
group_votes = {}
for method_groups in all_selected_groups:
    for group in method_groups:
        group_votes[group] = group_votes.get(group, 0) + 1

consensus_voting = [group for group, votes in group_votes.items()
                   if votes >= min_votes_threshold]

# Method 3: Weighted combination
group_scores = calculate_weighted_scores(all_selected_groups)
consensus_weighted = select_top_n_by_score(group_scores, n_groups)
```

**Parameters:**
- Selection methods to combine (3-5 complementary approaches)
- Consensus rule (intersection, voting, weighted scoring)
- Voting threshold (default: selected by â‰¥2 methods)
- Weighting scheme for different methods

**Pros:**
- âœ… **Robust selection** - Groups validated across multiple approaches
- âœ… **Method validation** - Cross-verification of different selection criteria
- âœ… **Uncertainty reduction** - Combines strengths of different methods
- âœ… **Confidence enhancement** - Higher trust in consensus-selected groups
- âœ… **Comprehensive evaluation** - Considers multiple selection criteria

**Cons:**
- âŒ **Computational cost** - Running multiple selection methods
- âŒ **Result integration complexity** - Challenging to combine different outputs
- âŒ **Consensus rule arbitrariness** - Different rules give different results
- âŒ **Conservative bias** - Strict consensus may miss valid groups
- âŒ **Parameter harmonization** - Different methods may have conflicting parameters

**Limitations:**
- Requires careful selection of which methods to combine
- Consensus criteria are somewhat subjective
- May miss biologically important groups unique to specific methods
- Interpretation becomes more complex with multiple criteria

**Why Choose:** Provides high-confidence group selection with validation across multiple complementary approaches.

**Expected Results:** Selects 3-6 groups with strong consensus support across selection methods

---

### **10. Adaptive Group Selection**
**Description:** Iteratively refine group selection based on model performance and clinical relevance metrics.

**Algorithm:**
```python
def adaptive_group_selection(grouped_features, data, clinical_vars, target_n_groups=5):
    current_groups = grouped_features.copy()
    best_score = -float('inf')
    best_selection = None

    for iteration in range(max_iterations):
        # Generate candidate selections
        candidate_selections = generate_candidate_selections(current_groups, target_n_groups)

        for candidate in candidate_selections:
            # Evaluate candidate
            model_score = evaluate_group_set(candidate, data, clinical_vars)
            clinical_score = evaluate_clinical_relevance(candidate, data)
            stability_score = evaluate_stability(candidate, data)

            # Combined score
            total_score = combine_scores(model_score, clinical_score, stability_score)

            if total_score > best_score:
                best_score = total_score
                best_selection = candidate

        # Refine search space based on best candidate
        current_groups = refine_search_space(best_selection, current_groups)

        # Check convergence
        if convergence_criteria_met(best_score, iteration):
            break

    return best_selection
```

**Parameters:**
- Target number of groups (default: 3-8)
- Evaluation metrics (model performance, clinical relevance, stability)
- Score combination weights
- Convergence criteria (score improvement threshold, max iterations)
- Search refinement strategy

**Pros:**
- âœ… **Performance optimization** - Finds group combinations maximizing predictive performance
- âœ… **Adaptive refinement** - Learns from evaluation feedback
- âœ… **Multi-objective optimization** - Balances model fit, clinical relevance, and stability
- âœ… **Automated discovery** - Finds optimal group combinations without manual trial-and-error
- âœ… **Comprehensive evaluation** - Considers multiple selection criteria simultaneously

**Cons:**
- âŒ **Computational complexity** - Iterative optimization with many evaluations
- âŒ **Parameter dependence** - Many parameters affect optimization outcome
- âŒ **Local optima risk** - May get stuck in suboptimal solutions
- âŒ **Evaluation metric selection** - Choice of metrics significantly affects results
- âŒ **Overfitting potential** - May optimize for evaluation metrics rather than generalizability

**Limitations:**
- Requires clear evaluation metrics and their relative importance
- May be computationally prohibitive for large group sets
- Results depend heavily on optimization parameters
- Validation becomes critical for adaptive selections

**Why Choose:** Advanced approach for finding optimal group combinations that maximize multiple objectives simultaneously.

**Expected Results:** Selects 3-8 optimally performing group combinations

---

## ðŸŽ¯ Implementation Strategy

### **Recommended Selection Pipeline:**
```python
# Phase 1: Initial screening
significant_groups = univariate_group_screening(grouped_features, data, p_threshold=0.1)

# Phase 2: Clinical context validation
clinical_groups = multivariate_group_selection(significant_groups, data, clinical_vars, p_threshold=0.05)

# Phase 3: Robustness validation
stable_groups = stability_group_selection(clinical_groups, data, stability_threshold=0.7)

# Phase 4: Final optimization
if len(stable_groups) > target_n_groups:
    final_groups = correlation_based_deduplication(stable_groups, data, target_n_groups)
else:
    final_groups = stable_groups
```

### **Success Metrics:**
- **PFS Predictive Power:** Selected groups improve PFS model performance
- **Clinical Relevance:** Groups correlate with meaningful clinical variables
- **Stability:** Groups show consistent associations across subsamples
- **Biological Interpretability:** Groups have clear microbial or functional meaning
- **Model Convergence:** Selected groups enable stable multivariate modeling

### **Validation Approach:**
- **Cross-method consistency:** Verify groups selected by multiple methods
- **Predictive validation:** Test group performance on held-out data
- **Clinical correlation:** Ensure groups relate to clinical outcomes beyond PFS
- **Sensitivity analysis:** Test robustness to selection parameter changes

This group selection phase ensures only the most clinically relevant and statistically robust microbial composites are carried forward to final PFS correlation analysis.